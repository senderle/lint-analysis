{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating global word count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "mpl.style.use('seaborn-muted')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enderlej/.virtualenvs/th/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "import os\n",
    "import ujson\n",
    "import attr\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from glob import glob\n",
    "from collections import Counter, UserDict\n",
    "from itertools import islice\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial count -- for weeding out words that never\n",
    "#                  appear in any novel ten or more times\n",
    "\n",
    "data_path = 'data/novels.100.json'\n",
    "\n",
    "def record_to_count(record):\n",
    "    tokens = record['token']\n",
    "    return Counter([t.lower() for t in tokens])\n",
    "\n",
    "_u_alpha = re.compile('^[\\w.,\"\\'?!;:]+$', re.UNICODE)\n",
    "def save_record(record, outpath='counts', \n",
    "                u_alpha=_u_alpha.match):\n",
    "    id = record['identifier']\n",
    "    outpath = os.path.join(outpath, '{}.csv'.format(id))\n",
    "    with open(outpath, 'w', encoding='utf-8') as op:\n",
    "        for w, ct in record_to_count(record).most_common():\n",
    "            if ct < 10:\n",
    "                break\n",
    "            if u_alpha(w):\n",
    "                op.write('{}\\t{}\\n'.format(w.replace('\\t', '\\\\t'), ct))\n",
    "\n",
    "def json_part_map(func, part):\n",
    "    try:\n",
    "        with bz2.open(part) as ip:\n",
    "            for line in ip:\n",
    "                yield func(ujson.loads(line))\n",
    "    except EOFError:\n",
    "        print('Error on file {}'.format(part))        \n",
    "        return\n",
    "\n",
    "all20 = [os.path.join(data_path, f) \n",
    "         for f in os.listdir(data_path)\n",
    "         if f.endswith('.bz2')]\n",
    "for f in all20:\n",
    "    _x = list(json_part_map(save_record, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second count -- for generating vectors of counts of \n",
    "#                 the 10k most common words by document\n",
    "#                 frequency from among the words selected\n",
    "#                 in the initial count.\n",
    "\n",
    "data_path = 'data/novels.100.json'\n",
    "words_path = 'counts/10kwords.txt'\n",
    "\n",
    "def load_words(path):\n",
    "    with open(path, encoding='utf-8') as ip:\n",
    "        return [w.strip() for w in ip]\n",
    "\n",
    "def record_to_count_vector(record, words):\n",
    "    tokens = record['token']\n",
    "    ct = Counter([t.lower() for t in tokens])\n",
    "    return np.array([ct[w] for w in words])\n",
    "\n",
    "def part_record_iter(part):\n",
    "    try:\n",
    "        with bz2.open(part) as ip:\n",
    "            for line in ip:\n",
    "                yield ujson.loads(line)\n",
    "    except EOFError:\n",
    "        print('Error on file {}'.format(part))        \n",
    "        return\n",
    "\n",
    "_u_alpha = re.compile('^[\\w.,\"\\'?!;:]+$', re.UNICODE)\n",
    "def part_to_npz(partpath, words):\n",
    "    outpath = partpath.replace('json.bz2', 'npz')\n",
    "    arrays = {}\n",
    "    for r in part_record_iter(partpath):\n",
    "        arrays[r['identifier']] = record_to_count_vector(r, words)\n",
    "    \n",
    "    np.savez_compressed(outpath, **arrays)\n",
    "\n",
    "all20 = [os.path.join(data_path, f) \n",
    "         for f in os.listdir(data_path)\n",
    "         if f.endswith('.bz2')]\n",
    "words = load_words(words_path)\n",
    "for f in all20:\n",
    "    part_to_npz(f, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
