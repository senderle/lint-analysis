{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A vs. the - next-word log-likelihood ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Count up which words come after \"a\" and \"the\" in the first and last 5% of novels. This gives 4 sets of counts - `a_beginning`, `a_end`, `the_beginning`, `the_end`.\n",
    "- Then, using Dunning's log-likelihood ratio, we can get words that follow a/the distinctively in one of these contexts. Eg, words that follow \"a\" distinctively in the first 5% relative to \"the.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import attr\n",
    "import os\n",
    "import ujson\n",
    "import bz2\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from itertools import islice\n",
    "from functools import partial\n",
    "from boltons.iterutils import pairwise\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from scipy import stats\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_offset(seq, skip=0):\n",
    "    \"\"\"Yield (item, 0-1 offset).\n",
    "    \"\"\"\n",
    "    size = len(seq)\n",
    "    start = max(0, int(size * skip) - 10)\n",
    "    for i in range(start, size):\n",
    "        item = seq[i]\n",
    "        offset = i / (size - 1) if (size - 1) else 0\n",
    "        yield item, offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_segment(func, path):\n",
    "    \"\"\"Parse JSON segment, apply worker function.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    with bz2.open(path) as fh:\n",
    "        for line in fh:\n",
    "            results.append(func(ujson.loads(line)))\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@attr.s\n",
    "class Corpus:\n",
    "    \n",
    "    root = attr.ib()\n",
    "    \n",
    "    def paths(self):\n",
    "        return glob(os.path.join(self.root, '*.bz2'))\n",
    "                        \n",
    "    def map_novels(self, func, shuffle=True):\n",
    "        \"\"\"Apply a worker to segment files in parallel.\n",
    "        \"\"\"\n",
    "        paths = self.paths()\n",
    "        \n",
    "        if shuffle:\n",
    "            random.shuffle(paths)\n",
    "        \n",
    "        with Pool() as p:\n",
    "            \n",
    "            worker = partial(map_segment, func)\n",
    "            \n",
    "            for results in p.imap_unordered(worker, paths):\n",
    "                yield from results\n",
    "\n",
    "def load_vocab(path):\n",
    "    with open(path) as ip:\n",
    "        return set(w.strip() for w in ip)\n",
    "\n",
    "def save_vocab(words):\n",
    "    with open(os.path.join(data_dir, 'dunnings-vocab.txt'), 'w') as op:\n",
    "        for w in words:\n",
    "            op.write('{}\\n'.format(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../../data'\n",
    "corpus = Corpus(os.path.join(data_dir, 'chicago-bins-tokens.json'))\n",
    "vocab = load_vocab(os.path.join(data_dir, 'dunnings-vocab.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suffixes_worker(q, o1, o2, vocab, n):\n",
    "    results = []\n",
    "    \n",
    "    for (t1, t2), offset in zip_offset(pairwise(n['tokens']), skip=o1):\n",
    "        if o1 < offset < o2 and t2.lower() in vocab:\n",
    "            results.append(t2.lower())\n",
    "            if t1.lower() == q:\n",
    "                results.append('{}_{}'.format(q, t2.lower()))\n",
    "\n",
    "        elif offset >= o2:\n",
    "            break\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suffixes(q, o1, o2, num_novels=None):\n",
    "    worker = partial(suffixes_worker, q, o1, o2, set(vocab))\n",
    "    res_iter = islice(corpus.map_novels(worker), num_novels)\n",
    "    return Counter([m for ms in tqdm(res_iter) for m in ms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6638it [02:42, 40.84it/s]\n"
     ]
    }
   ],
   "source": [
    "a0_combined = suffixes('a', 0, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6638it [02:33, 43.18it/s]\n"
     ]
    }
   ],
   "source": [
    "a1_combined = suffixes('a', 0.95, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6638it [02:42, 40.75it/s]\n"
     ]
    }
   ],
   "source": [
    "the0_combined = suffixes('the', 0, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6638it [02:35, 42.70it/s]\n"
     ]
    }
   ],
   "source": [
    "the1_combined = suffixes('the', 0.95, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0_ = {w: a0_combined['a_' + w] for w in vocab}\n",
    "a1_ = {w: a1_combined['a_' + w] for w in vocab}\n",
    "the0_ = {w: the0_combined['the_' + w] for w in vocab}\n",
    "the1_ = {w: the1_combined['the_' + w] for w in vocab}\n",
    "total0_ = {w: a0_combined[w] for w in vocab}\n",
    "total1_ = {w: a1_combined[w] for w in vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdw(fg, bg, min_count=100, n=50):\n",
    "\n",
    "    vocab = set.intersection(\n",
    "        {t for t, c in fg.items() if c > min_count},\n",
    "        {t for t, c in bg.items() if c > min_count},\n",
    "    )\n",
    "    \n",
    "    n_fg = sum(fg[t] for t in vocab)\n",
    "    n_bg = sum(bg[t] for t in vocab)\n",
    "    \n",
    "    rows = []\n",
    "    for t in vocab:\n",
    "        \n",
    "        p = (fg[t] + bg[t]) / (n_fg + n_bg)\n",
    "        \n",
    "        e_fg = n_fg * p\n",
    "        e_bg = n_bg * p\n",
    "        \n",
    "        if fg[t] > e_fg:\n",
    "\n",
    "            s, _ = stats.power_divergence(\n",
    "                [fg[t], bg[t]],\n",
    "                [e_fg, e_bg],\n",
    "                lambda_='log-likelihood',\n",
    "            )\n",
    "\n",
    "            rows.append((t, s))\n",
    "            \n",
    "    return pd.DataFrame(rows, columns=('token', 'dll'))\n",
    "\n",
    "# def weighted_pmi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_a_w_0_ = {w: a0_[w] / total0_[w] for w in a0_}\n",
    "p_a_w_1_ = {w: a1_[w] / total1_[w] for w in a1_}\n",
    "p_a_w = {w: (a0_[w] + a1_[w]) / (total0_[w] + total1_[w]) for w in a0_}\n",
    "ll_a_w_0 = {w: p_a_w_0_[w] * -math.log(p_a_w_0_[w] / p_a_w[w]) for w in a0_}\n",
    "ll_a_w_1 = {w: p_a_w_1_[w] * -math.log(p_a_w_1_[w] / p_a_w[w]) for w in a1_}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Words with falling \"a\" probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word                          p(a|w,0)       p(a|w,1)\n",
      "----                          --------       --------\n",
      "gun                           0.21117        0.13216\n",
      "witch                         0.29346        0.19022\n",
      "penny                         0.25561        0.14901\n",
      "pistol                        0.19159        0.12097\n",
      "clue                          0.49312        0.39568\n",
      "cave                          0.17525        0.10055\n",
      "sword                         0.16243        0.09765\n",
      "flood                         0.22403        0.15691\n",
      "hammer                        0.19140        0.12687\n",
      "dragon                        0.16003        0.09927\n",
      "murder                        0.11549        0.06371\n",
      "flurry                        0.61236        0.54106\n",
      "snake                         0.29062        0.22254\n",
      "former                        0.18315        0.11275\n",
      "hurry                         0.27707        0.21004\n",
      "knife                         0.21426        0.16021\n",
      "pity                          0.24027        0.18301\n",
      "fight                         0.14887        0.09628\n",
      "shotgun                       0.21158        0.16634\n",
      "blade                         0.09183        0.04847\n",
      "widow                         0.24340        0.16820\n",
      "rifle                         0.16539        0.12524\n",
      "major                         0.21291        0.15669\n",
      "funeral                       0.14339        0.09989\n",
      "certain                       0.30364        0.24992\n",
      "bull                          0.17405        0.12263\n",
      "bullet                        0.29343        0.25728\n",
      "constant                      0.21289        0.15411\n",
      "needle                        0.19482        0.14820\n",
      "statue                        0.22041        0.17768\n",
      "baby                          0.19210        0.14795\n",
      "weak                          0.15833        0.11798\n",
      "genius                        0.27719        0.21667\n",
      "wedding                       0.15134        0.11227\n",
      "decent                        0.41763        0.36820\n",
      "permanent                     0.32469        0.27598\n",
      "shock                         0.17716        0.13956\n",
      "brand                         0.24606        0.20376\n",
      "novel                         0.23163        0.18925\n",
      "prison                        0.11638        0.08219\n",
      "trace                         0.34307        0.29854\n",
      "mental                        0.24617        0.19958\n",
      "flashlight                    0.22992        0.19441\n",
      "toy                           0.28885        0.24760\n",
      "generous                      0.20548        0.16064\n",
      "detective                     0.14770        0.10432\n",
      "gentle                        0.23239        0.19189\n",
      "faint                         0.40295        0.36223\n",
      "trifle                        0.89617        0.84615\n",
      "loose                         0.10815        0.07123\n"
     ]
    }
   ],
   "source": [
    "print('word                          p(a|w,0)       p(a|w,1)')\n",
    "print('----                          --------       --------')\n",
    "for w in sorted(ll_a_w_0, key=ll_a_w_0.get)[:50]:\n",
    "    print('{:30}{:1.5f}        {:1.5f}'.format(w, p_a_w_0_[w], p_a_w_1_[w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Words with rising \"a\" probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word                          p(a|w,0)       p(a|w,1)\n",
      "----                          --------       --------\n",
      "puff                          0.32515        0.46875\n",
      "mouthful                      0.66805        0.77564\n",
      "stranger                      0.35350        0.45261\n",
      "rag                           0.26096        0.36695\n",
      "headache                      0.34152        0.44970\n",
      "thief                         0.28850        0.38734\n",
      "football                      0.17208        0.25285\n",
      "fake                          0.21020        0.32037\n",
      "nap                           0.38692        0.48101\n",
      "hero                          0.27343        0.38383\n",
      "wry                           0.51071        0.60294\n",
      "divorce                       0.20537        0.28660\n",
      "fleeting                      0.38667        0.46484\n",
      "victim                        0.09919        0.16714\n",
      "pencil                        0.25796        0.32143\n",
      "tangle                        0.30072        0.37568\n",
      "vacation                      0.20597        0.27048\n",
      "momentary                     0.45210        0.52303\n",
      "lesson                        0.27861        0.34923\n",
      "waiter                        0.14745        0.20300\n",
      "tour                          0.15784        0.21034\n",
      "brick                         0.08617        0.13108\n",
      "deer                          0.13891        0.19067\n",
      "lousy                         0.29128        0.35142\n",
      "trap                          0.20174        0.27766\n",
      "while                         0.15796        0.21859\n",
      "taxi                          0.32255        0.38171\n",
      "massive                       0.23476        0.28693\n",
      "shower                        0.24651        0.30105\n",
      "mighty                        0.21890        0.27794\n",
      "blur                          0.37740        0.44186\n",
      "third                         0.18238        0.23271\n",
      "sudden                        0.44925        0.50914\n",
      "formal                        0.16429        0.21207\n",
      "period                        0.16763        0.21569\n",
      "towel                         0.24118        0.29135\n",
      "groan                         0.34049        0.39944\n",
      "rare                          0.23848        0.27974\n",
      "flower                        0.15671        0.20582\n",
      "stool                         0.19909        0.24324\n",
      "photograph                    0.22427        0.27303\n",
      "team                          0.09654        0.13943\n",
      "deep                          0.24426        0.29617\n",
      "remarkable                    0.20658        0.25475\n",
      "long                          0.23129        0.28182\n",
      "cigarette                     0.37896        0.42259\n",
      "cheap                         0.17677        0.21478\n",
      "gentleman                     0.26675        0.30566\n",
      "jerk                          0.22134        0.26923\n",
      "split                         0.12703        0.17517\n"
     ]
    }
   ],
   "source": [
    "print('word                          p(a|w,0)       p(a|w,1)')\n",
    "print('----                          --------       --------')\n",
    "for w in sorted(ll_a_w_1, key=ll_a_w_1.get)[:50]:\n",
    "    print('{:30}{:05.5f}        {:5.5f}'.format(w, p_a_w_0_[w], p_a_w_1_[w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a > the (beginning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'few little lot good bit couple moment long while hundred small dozen minute single large week pair half thousand nice month year piece different quick smile fine short pretty very hand bad drink deep series chance fool quarter slight look sudden brief man cigarette step strange special child full great finger rather strong mistake private beautiful pleasant wonderful joke new woman loud person real note hint sense wave copy big cup mere thin simple - sharp lovely low thing handsome huge terrible faint number row part better fresh damn complete tight curious tall vague breath visit professional happy flash personal'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(mdw(a0_, the0_).sort_values('dll', ascending=False).head(100).token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the > a (beginning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first door last back world house rest front room two way water street most kitchen road one top people window sky morning wind table city middle right night river wall phone side time car bed sea fact rain town whole king land police beach hell war building village crowd doorway desk three sound smell bar blood church light hill thought far body head bridge lake country work ship fire sight train corner yard name company power boy store hospital future pain forest scene hotel queen radio boat dark summer bank law garden mirror family path winter telephone spring shoulder coffee'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(mdw(the0_, a0_).sort_values('dll', ascending=False).head(100).token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a > the (end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'few little lot moment good long while couple minute hundred small single week deep friend large half pair hand chance piece sudden drink smile step man very great quick short month year fine fool pretty new bad look strange mistake brief woman cigarette slight finger lie quarter wonderful terrible low part child better loud special sense dream beautiful strong huge big private damn - copy thing joke full breath cup kind simple stranger hero tight wave flash person faint hard sharp word slow thin lovely fresh miracle fair shot note break kiss happy number visit curious mere perfect clear gesture'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(mdw(a1_, the1_).sort_values('dll', ascending=False).head(100).token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the > a (end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'door first room house back last two rest world one front water road way sky people kitchen night top street window morning wind most police bed middle side table right city wall car river phone rain whole sea crowd pain doorway three fact hospital beach fire blood church hell head king hill power bridge building land desk light time future body sound dark town sight law gun country lake judge bar couch village thought far ship garden hotel work yard truck queen war four forest smell boat radio face boy scene name blade bank living stage company doctor telephone corner'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(mdw(the1_, a1_).sort_values('dll', ascending=False).head(100).token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- \"A\" is clearly used in the context of description - many of the distinctive a>the words are adjectives, whereas in the the>a lists, nouns.\n",
    "- Of the a>the adjectives, interesting that many of the most distinctive ones are quantifiers, markers of _degree_ - few, little, lot, bit, hundred, single, small, large.\n",
    "- \"A\" also associated with time, questions of when / how long - while, minute, week, year.\n",
    "- \"The,\" meanwhile, is clearly marking what might be thought of as \"physical rendering\" - descriptions of physical settings and spatial relationships. \"Sides\" of things - front, back, middle, side; and literal objects and locations - door, floor, house, room, street, kitchen, road, etc.\n",
    "- So basically, my gloss - \"a\" is description (and temporality?), \"the\" is physicality. At the beginning, both are common - things are getting introduced for the first time, the physical setting is getting established. Whereas, at the end, there's a return to the physical (away from the psychological / dialogic middle?), with \"the\" going back up; but, less need for \"a,\" since the world has already been described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Less useful - \"a\" (beginning) compared to \"a\" (end), vice versa, and \"the\" vs \"the.\" This basically just reproduces the overall frequency differences, though. (Murder at the end, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'year tall young large boy girl small well pair half handsome month student “ week two high narrow thin big town face lady white three city pleasant brown broad widow job name fat house four - slender six black decade blue thick private school beauty five habit natural day row gold summer square dollar first female wide successful country dark husband dozen constant century street teacher fine block pale local certain particularly slim genius frown model detective twenty slight famous delicate friendly slightly kid variety particular bar social shade middle cow quarter full vague land writer rather living rare desk'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(mdw(a0_, a1_).sort_values('dll', ascending=False).head(100).token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'moment while long gun murderer minute chance step shot lie bullet terrible deal way hero last deep message lot hand letter little witness roar trap bitch voice dream final weapon sudden time nurse second kiss pistol fool part plan sound cry killer chair whisper scream great trial promise noise horrible choice new rifle fist traitor hug monster fake wonderful decision tear breath blanket flash shotgun guard silence will criminal tremendous word suicide life liar thing move sitting mistake grave split question death desperate few deadly crazy trick blessing prayer plane shadow court miracle saint brave search difference threat light fucking'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(mdw(a1_, a0_).sort_values('dll', ascending=False).head(100).token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'town girl young family boy year city summer school stranger man local most village “ younger name street big woman store wide high teacher victim sort tall more class bar - small thick kind narrow gentleman war land large rich middle late broad job thin winter few guy daughter smell third wagon male habit fact bus spring mirror week lady fine business century home french great company low table five long day hot half country waiter british club social farm famous planet heavy national dusty ten wife kid kitchen fourth roman word view foreign problem widow modern visitor smaller sign'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(mdw(the0_, the1_).sort_values('dll', ascending=False).head(100).token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gun pistol police knife door murder hospital rifle bullet murderer pain sword shotgun bed dragon cave fire room shot blade baby judge story grave tunnel one night weapon future car battle final nurse light sky key rope will couch blow bomb hammer trial flood force power wedding circle whole way funeral truck rest fight sound love cell cliff case stone death people demon ring side blood doctor crowd shock note mountain witness doorway hell hole statue chain broken witch chance wall bridge last plan letter flashlight roar box distance burning suitcase storm court machine real rock monster spear terrible spot'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(mdw(the1_, the0_).sort_values('dll', ascending=False).head(100).token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
